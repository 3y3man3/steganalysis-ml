{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4735c34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from feature_extraction.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pywt\\_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Image.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, metrics, preprocessing, linear_model, neighbors, ensemble, neural_network\n",
    "# from \"feature_extraction.ipynb\" import get_feature_vector\n",
    "import import_ipynb\n",
    "from feature_extraction import get_feature_vector\n",
    "import glob\n",
    "import fleep\n",
    "from Image import Image\n",
    "import ast\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fd6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_svm_classifier(csv_file, joblib_file):\n",
    "    '''\n",
    "        Train SVM classifier with training set (feature vectors) from csv file,\n",
    "        and load the trained model in .joblib file\n",
    "    '''\n",
    "    training_data = pandas.read_csv(csv_file) #import our training data from the csv file\n",
    "    \n",
    "    x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler.fit_transform(x) #scale the features in the interval [0:1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #80% for training, 20% for testing\n",
    "    \n",
    "    classifier = svm.SVC(kernel = 'poly')\n",
    "    classifier.fit(x_train, y_train) # fit svm classifier to the train data\n",
    "    y_prediction = classifier.predict(x_test)\n",
    "        \n",
    "#     accuracy = metrics.accuracy_score(y_test, y_prediction) # accuracy of our model: number of correct predictions / number of total predictions\n",
    "    print(\"Accuracy on train set: \", classifier.score(x_train, y_train))\n",
    "    print(\"Accuracy on test set: \", classifier.score(x_test, y_test))\n",
    "#     print(\"Accuracy on test set: \", accuracy)\n",
    "#     joblib.dump(classifier, joblib_file) # save classifier in the joblib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a40d812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knn_classifier(csv_file, joblib_file):\n",
    "    '''\n",
    "        Train SVM classifier with training set (feature vectors) from csv file,\n",
    "        and load the trained model in .joblib file\n",
    "    '''\n",
    "    training_data = pandas.read_csv(csv_file) #import our training data from the csv file\n",
    "    \n",
    "    x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler.fit_transform(x) #scale the features in the interval [0:1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #80% for training, 20% for testing\n",
    "    \n",
    "    classifier = neighbors.KNeighborsClassifier(100)\n",
    "    classifier.fit(x_train, y_train) # fit svm classifier to the train data\n",
    "    y_prediction = classifier.predict(x_test)\n",
    "    \n",
    "    \n",
    "#     accuracy = metrics.accuracy_score(y_test, y_prediction) # accuracy of our model: number of correct predictions / number of total predictions\n",
    "    print(\"K-nearest neighbors classifier: \")\n",
    "    print(\"Accuracy on train set: \", classifier.score(x_train, y_train))\n",
    "    print(\"Accuracy on test set: \", classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a6c7101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_forest_classifier(csv_file, joblib_file):\n",
    "    '''\n",
    "        Train SVM classifier with training set (feature vectors) from csv file,\n",
    "        and load the trained model in .joblib file\n",
    "    '''\n",
    "    training_data = pandas.read_csv(csv_file) #import our training data from the csv file\n",
    "    \n",
    "    x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler.fit_transform(x) #scale the features in the interval [0:1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #80% for training, 20% for testing\n",
    "    \n",
    "    classifier = ensemble.RandomForestClassifier(max_depth=7, n_estimators=20, max_features=100)\n",
    "    classifier.fit(x_train, y_train) # fit svm classifier to the train data\n",
    "    y_prediction = classifier.predict(x_test)\n",
    "    \n",
    "    \n",
    "#     accuracy = metrics.accuracy_score(y_test, y_prediction) # accuracy of our model: number of correct predictions / number of total predictions\n",
    "    print(\"Random Forests classifier: \")\n",
    "    print(\"Accuracy on train set: \", classifier.score(x_train, y_train))\n",
    "    print(\"Accuracy on test set: \", classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2848e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_classifier(csv_file, joblib_file):\n",
    "    '''\n",
    "        Train SVM classifier with training set (feature vectors) from csv file,\n",
    "        and load the trained model in .joblib file\n",
    "    '''\n",
    "    training_data = pandas.read_csv(csv_file) #import our training data from the csv file\n",
    "    \n",
    "    x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler.fit_transform(x) #scale the features in the interval [0:1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #80% for training, 20% for testing\n",
    "    \n",
    "    classifier = neural_network.MLPClassifier(alpha=1, max_iter=1000)\n",
    "    classifier.fit(x_train, y_train) # fit svm classifier to the train data\n",
    "    y_prediction = classifier.predict(x_test)\n",
    "    \n",
    "    \n",
    "#     accuracy = metrics.accuracy_score(y_test, y_prediction) # accuracy of our model: number of correct predictions / number of total predictions\n",
    "    print(\"MLP NN classifier: \")\n",
    "    print(\"Accuracy on train set: \", classifier.score(x_train, y_train))\n",
    "    print(\"Accuracy on test set: \", classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3caa11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_update_features(image):\n",
    "    '''\n",
    "        Get feature vector of image object,\n",
    "        return image object with image.features updated\n",
    "    '''\n",
    "    feature_vector = get_feature_vector(image.name)\n",
    "    farid_r, farid_g, farid_b = feature_vector[0], feature_vector[1], feature_vector[2]\n",
    "    \n",
    "    farid_dict = {} # will have every feature in every channel\n",
    "    \n",
    "    counter = 1\n",
    "    for feature_value in farid_r:\n",
    "        feature_name = 'farid_r_{}'.format(counter)\n",
    "#         farid_dict[feature_name] = float(feature_value)\n",
    "        farid_dict[feature_name] = float(feature_value)\n",
    "        counter = counter + 1\n",
    "    counter = 1\n",
    "    for feature_value in farid_g:\n",
    "        feature_name = 'farid_g_{}'.format(counter)\n",
    "        farid_dict[feature_name] = float(feature_value)\n",
    "        counter = counter + 1\n",
    "    counter = 1\n",
    "    for feature_value in farid_b:\n",
    "        feature_name = 'farid_b_{}'.format(counter)\n",
    "        farid_dict[feature_name] = float(feature_value)\n",
    "        counter = counter + 1\n",
    "    \n",
    "    image.features.update(farid_dict)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ba784be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_images_list(img_list):\n",
    "    '''\n",
    "        Updates image.feature for a list of images\n",
    "    '''\n",
    "    for image in img_list:\n",
    "        image = extract_update_features(image)\n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cfa074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_lists(dir_location):\n",
    "    img_names = glob.glob(\"{}/*\".format(dir_location)) # gets list of all images in image directory (/stego or /clean)\n",
    "    img_list = [] # list of image objects\n",
    "    for img_name in img_names:\n",
    "#         if find_file(file_name):  # try to find file, if file can be found:\n",
    "        img_extension = get_img_extension(img_name)\n",
    "        img_size = os.path.getsize(img_name)\n",
    "        new_image = Image(img_name, img_extension, img_size)  # create Image object\n",
    "#         img_extension = get_img_extension(img_name)  # get file type and file extension\n",
    "#         img_size = os.path.getsize(img_name)  # get file size\n",
    "#         new_image.update_img(img_extension, img_size)  # update new_file with new info\n",
    "        img_list.append(new_image)  # add image object to image list\n",
    "    return img_list\n",
    "dire = os.getcwd()+'\\\\images'\n",
    "# get_img_lists(dire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e333d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_extension(img_name):\n",
    "#     with open(img_name, 'rb') as img:\n",
    "#         img_info = fleep.get(img.read(128))\n",
    "#         img_extension = img_info.extension[0]\n",
    "#         img.close()\n",
    "    name, img_extension = os.path.splitext(img_name)\n",
    "    return img_extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "599c207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dir_location):\n",
    "    '''\n",
    "        return two lists of image objects in '/stego' and '/clean' directories with updated features\n",
    "    '''\n",
    "    # get image object list for stego and clean\n",
    "    stego_images = get_img_lists(\"{}/stego-reduced\".format(dir_location))\n",
    "    clean_images = get_img_lists(\"{}/clean-reduced\".format(dir_location))\n",
    "    \n",
    "    #extract and update features for both lists\n",
    "    stego_with_features = extract_from_images_list(stego_images)\n",
    "    clean_with_features = extract_from_images_list(clean_images)\n",
    "    \n",
    "    return stego_with_features, clean_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a821fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_img_to_csv(stego_with_features, clean_with_features):\n",
    "    output_file = 'img-features.csv' # our csv output file\n",
    "\n",
    "    list_of_dicts = []\n",
    "    feature_types = []\n",
    "    \n",
    "    for img in stego_with_features:\n",
    "        temp_dict = {}\n",
    "        temp_dict['img_name'] = img.name\n",
    "        for feature_type, feature_list in img.features.items():\n",
    "            temp_dict[feature_type] = feature_list\n",
    "            if feature_type not in feature_types:\n",
    "                feature_types.append(feature_type)\n",
    "        temp_dict['class'] = 1\n",
    "        list_of_dicts.append(temp_dict)\n",
    "    \n",
    "    for img in clean_with_features:\n",
    "        temp_dict = {}\n",
    "        temp_dict['img_name'] = img.name\n",
    "        for feature_type, feature_list in img.features.items():\n",
    "            temp_dict[feature_type] = feature_list\n",
    "            if feature_type not in feature_types:\n",
    "                feature_types.append(feature_type)\n",
    "        temp_dict['class'] = 0\n",
    "        list_of_dicts.append(temp_dict)\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        # set fieldnames\n",
    "        fieldnames = ['img_name']\n",
    "        for feature_type in feature_types:\n",
    "            fieldnames.append(feature_type)\n",
    "        fieldnames.append('class')\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for dict_item in list_of_dicts:\n",
    "            writer.writerow(dict_item)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df2c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dir_location):\n",
    "    '''\n",
    "        Extrat features, create, train, and test our model\n",
    "    '''\n",
    "    directory = \"{}/images\".format(dir_location)\n",
    "    \n",
    "    stego_with_features, clean_with_features = extract_features(directory)\n",
    "    \n",
    "    write_img_to_csv(stego_with_features, clean_with_features)\n",
    "    \n",
    "    create_svm_classifier('img-features.csv', 'img-svm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d3950d37",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['img_name' 'class'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36180/423349328.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train_model(os.getcwd())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcreate_svm_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'img-features.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'img-svm.joblib'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# create_knn_classifier('img-features.csv', 'img-svm.joblib')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# create_random_forest_classifier('img-features.csv', 'img-svm.joblib')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# create_mlp_classifier('img-features.csv', 'img-svm.joblib')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36180/1483578573.py\u001b[0m in \u001b[0;36mcreate_svm_classifier\u001b[1;34m(csv_file, joblib_file)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#import our training data from the csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'img_name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4167\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4168\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4169\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4170\u001b[0m         )\n\u001b[0;32m   4171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3882\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3883\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3884\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3916\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3917\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3918\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3919\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5277\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5278\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5279\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5280\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['img_name' 'class'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# train_model(os.getcwd())\n",
    "create_svm_classifier('img-features.csv', 'img-svm.joblib')\n",
    "# create_knn_classifier('img-features.csv', 'img-svm.joblib')\n",
    "# create_random_forest_classifier('img-features.csv', 'img-svm.joblib')\n",
    "# create_mlp_classifier('img-features.csv', 'img-svm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89750aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\base.py:253: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.3 when using version 0.20.4. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "loaded_model = joblib.load(\"img-lr-jessica.joblib\")\n",
    "\n",
    "training_data = pandas.read_csv('img-features.csv') #import our training data from the csv file\n",
    "x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "x = scaler.fit_transform(x) #scale the features in the interval [0:1]\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #80% for training, 20% for testing\n",
    "\n",
    "print(\"Accuracy on test set: \", loaded_model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0756ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e254cc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>farid_r_1</th>\n",
       "      <th>farid_r_2</th>\n",
       "      <th>farid_r_3</th>\n",
       "      <th>farid_r_4</th>\n",
       "      <th>farid_r_5</th>\n",
       "      <th>farid_r_6</th>\n",
       "      <th>farid_r_7</th>\n",
       "      <th>farid_r_8</th>\n",
       "      <th>farid_r_9</th>\n",
       "      <th>farid_r_10</th>\n",
       "      <th>...</th>\n",
       "      <th>farid_b_27</th>\n",
       "      <th>farid_b_28</th>\n",
       "      <th>farid_b_29</th>\n",
       "      <th>farid_b_30</th>\n",
       "      <th>farid_b_31</th>\n",
       "      <th>farid_b_32</th>\n",
       "      <th>farid_b_33</th>\n",
       "      <th>farid_b_34</th>\n",
       "      <th>farid_b_35</th>\n",
       "      <th>farid_b_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.495927</td>\n",
       "      <td>142.971994</td>\n",
       "      <td>-5.173403</td>\n",
       "      <td>105.572625</td>\n",
       "      <td>-16.245859</td>\n",
       "      <td>17866.879034</td>\n",
       "      <td>-0.089559</td>\n",
       "      <td>0.860824</td>\n",
       "      <td>0.025910</td>\n",
       "      <td>17.413688</td>\n",
       "      <td>...</td>\n",
       "      <td>3.697833</td>\n",
       "      <td>383.657296</td>\n",
       "      <td>-0.044727</td>\n",
       "      <td>5408.678979</td>\n",
       "      <td>-0.472284</td>\n",
       "      <td>0.401013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>3.066421</td>\n",
       "      <td>-0.789374</td>\n",
       "      <td>234.072874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.291454</td>\n",
       "      <td>165.408289</td>\n",
       "      <td>-0.650901</td>\n",
       "      <td>26.581393</td>\n",
       "      <td>-23.746580</td>\n",
       "      <td>27306.305584</td>\n",
       "      <td>0.088541</td>\n",
       "      <td>-0.825371</td>\n",
       "      <td>0.015227</td>\n",
       "      <td>20.122657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244798</td>\n",
       "      <td>30.730703</td>\n",
       "      <td>-0.015340</td>\n",
       "      <td>703.330467</td>\n",
       "      <td>-0.696145</td>\n",
       "      <td>6.483713</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>3.332348</td>\n",
       "      <td>0.051449</td>\n",
       "      <td>17.678393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.227207</td>\n",
       "      <td>2388.346164</td>\n",
       "      <td>-0.479972</td>\n",
       "      <td>6.258614</td>\n",
       "      <td>-16.485955</td>\n",
       "      <td>19792.141898</td>\n",
       "      <td>-0.086181</td>\n",
       "      <td>0.531394</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>290.413323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476321</td>\n",
       "      <td>26.142263</td>\n",
       "      <td>-0.032921</td>\n",
       "      <td>3055.638514</td>\n",
       "      <td>-0.459322</td>\n",
       "      <td>0.259293</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>69.413888</td>\n",
       "      <td>-0.101721</td>\n",
       "      <td>14.866676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.178336</td>\n",
       "      <td>99.592376</td>\n",
       "      <td>0.399630</td>\n",
       "      <td>66.128233</td>\n",
       "      <td>-15.367973</td>\n",
       "      <td>12686.086157</td>\n",
       "      <td>0.025283</td>\n",
       "      <td>-0.156699</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>12.113558</td>\n",
       "      <td>...</td>\n",
       "      <td>1.652424</td>\n",
       "      <td>247.799711</td>\n",
       "      <td>-0.034670</td>\n",
       "      <td>2892.223032</td>\n",
       "      <td>-0.459490</td>\n",
       "      <td>0.515741</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>3.949912</td>\n",
       "      <td>-0.348138</td>\n",
       "      <td>150.724222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.232871</td>\n",
       "      <td>865.751225</td>\n",
       "      <td>0.203080</td>\n",
       "      <td>11.688932</td>\n",
       "      <td>-11.577261</td>\n",
       "      <td>7909.909319</td>\n",
       "      <td>-0.073709</td>\n",
       "      <td>1.605954</td>\n",
       "      <td>0.012166</td>\n",
       "      <td>105.276101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638960</td>\n",
       "      <td>56.523797</td>\n",
       "      <td>-0.033231</td>\n",
       "      <td>2524.451998</td>\n",
       "      <td>-0.472095</td>\n",
       "      <td>0.821683</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>15.007591</td>\n",
       "      <td>-0.135635</td>\n",
       "      <td>33.490953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-0.302832</td>\n",
       "      <td>1323.506442</td>\n",
       "      <td>-0.109463</td>\n",
       "      <td>3.389517</td>\n",
       "      <td>-15.848878</td>\n",
       "      <td>15284.931603</td>\n",
       "      <td>-0.022001</td>\n",
       "      <td>0.105237</td>\n",
       "      <td>0.015822</td>\n",
       "      <td>160.940580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168602</td>\n",
       "      <td>10.450490</td>\n",
       "      <td>-0.049275</td>\n",
       "      <td>4988.911095</td>\n",
       "      <td>-0.390495</td>\n",
       "      <td>-0.611365</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>90.446884</td>\n",
       "      <td>0.036639</td>\n",
       "      <td>5.246472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-0.059291</td>\n",
       "      <td>445.337399</td>\n",
       "      <td>0.138710</td>\n",
       "      <td>15.705625</td>\n",
       "      <td>-7.851543</td>\n",
       "      <td>4450.732281</td>\n",
       "      <td>-0.232631</td>\n",
       "      <td>5.036927</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>54.150517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148542</td>\n",
       "      <td>12.313178</td>\n",
       "      <td>-0.022201</td>\n",
       "      <td>1386.488040</td>\n",
       "      <td>-0.591700</td>\n",
       "      <td>3.480437</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>28.440451</td>\n",
       "      <td>-0.031909</td>\n",
       "      <td>6.388110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.071814</td>\n",
       "      <td>1183.608183</td>\n",
       "      <td>0.257604</td>\n",
       "      <td>4.606061</td>\n",
       "      <td>-13.323997</td>\n",
       "      <td>13156.688947</td>\n",
       "      <td>-0.114114</td>\n",
       "      <td>1.023038</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>143.919584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286904</td>\n",
       "      <td>15.130832</td>\n",
       "      <td>-0.038789</td>\n",
       "      <td>3353.990490</td>\n",
       "      <td>-0.406398</td>\n",
       "      <td>-0.432799</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>39.679107</td>\n",
       "      <td>-0.061182</td>\n",
       "      <td>8.115477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.059123</td>\n",
       "      <td>667.963832</td>\n",
       "      <td>0.478477</td>\n",
       "      <td>22.049475</td>\n",
       "      <td>-9.184075</td>\n",
       "      <td>6685.999633</td>\n",
       "      <td>-0.238883</td>\n",
       "      <td>3.696547</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>81.220425</td>\n",
       "      <td>...</td>\n",
       "      <td>1.195385</td>\n",
       "      <td>165.950049</td>\n",
       "      <td>-0.022171</td>\n",
       "      <td>1690.748781</td>\n",
       "      <td>-0.644783</td>\n",
       "      <td>3.764329</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>5.870222</td>\n",
       "      <td>-0.255036</td>\n",
       "      <td>100.579733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-0.252067</td>\n",
       "      <td>375.542392</td>\n",
       "      <td>-0.041245</td>\n",
       "      <td>16.290231</td>\n",
       "      <td>-16.867456</td>\n",
       "      <td>16261.474510</td>\n",
       "      <td>-0.022210</td>\n",
       "      <td>0.464549</td>\n",
       "      <td>0.013169</td>\n",
       "      <td>45.671038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802114</td>\n",
       "      <td>80.567973</td>\n",
       "      <td>-0.037090</td>\n",
       "      <td>3536.945813</td>\n",
       "      <td>-0.488229</td>\n",
       "      <td>0.854156</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>9.187654</td>\n",
       "      <td>-0.168201</td>\n",
       "      <td>48.223144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     farid_r_1    farid_r_2  farid_r_3   farid_r_4  farid_r_5     farid_r_6  \\\n",
       "0    -0.495927   142.971994  -5.173403  105.572625 -16.245859  17866.879034   \n",
       "1    -0.291454   165.408289  -0.650901   26.581393 -23.746580  27306.305584   \n",
       "2    -0.227207  2388.346164  -0.479972    6.258614 -16.485955  19792.141898   \n",
       "3    -0.178336    99.592376   0.399630   66.128233 -15.367973  12686.086157   \n",
       "4    -0.232871   865.751225   0.203080   11.688932 -11.577261   7909.909319   \n",
       "..         ...          ...        ...         ...        ...           ...   \n",
       "195  -0.302832  1323.506442  -0.109463    3.389517 -15.848878  15284.931603   \n",
       "196  -0.059291   445.337399   0.138710   15.705625  -7.851543   4450.732281   \n",
       "197  -0.071814  1183.608183   0.257604    4.606061 -13.323997  13156.688947   \n",
       "198  -0.059123   667.963832   0.478477   22.049475  -9.184075   6685.999633   \n",
       "199  -0.252067   375.542392  -0.041245   16.290231 -16.867456  16261.474510   \n",
       "\n",
       "     farid_r_7  farid_r_8  farid_r_9  farid_r_10  ...  farid_b_27  farid_b_28  \\\n",
       "0    -0.089559   0.860824   0.025910   17.413688  ...    3.697833  383.657296   \n",
       "1     0.088541  -0.825371   0.015227   20.122657  ...   -0.244798   30.730703   \n",
       "2    -0.086181   0.531394   0.011871  290.413323  ...    0.476321   26.142263   \n",
       "3     0.025283  -0.156699   0.009317   12.113558  ...    1.652424  247.799711   \n",
       "4    -0.073709   1.605954   0.012166  105.276101  ...    0.638960   56.523797   \n",
       "..         ...        ...        ...         ...  ...         ...         ...   \n",
       "195  -0.022001   0.105237   0.015822  160.940580  ...   -0.168602   10.450490   \n",
       "196  -0.232631   5.036927   0.003098   54.150517  ...    0.148542   12.313178   \n",
       "197  -0.114114   1.023038   0.003752  143.919584  ...    0.286904   15.130832   \n",
       "198  -0.238883   3.696547   0.003089   81.220425  ...    1.195385  165.950049   \n",
       "199  -0.022210   0.464549   0.013169   45.671038  ...    0.802114   80.567973   \n",
       "\n",
       "     farid_b_29   farid_b_30  farid_b_31  farid_b_32  farid_b_33  farid_b_34  \\\n",
       "0     -0.044727  5408.678979   -0.472284    0.401013   -0.000008    3.066421   \n",
       "1     -0.015340   703.330467   -0.696145    6.483713   -0.000002    3.332348   \n",
       "2     -0.032921  3055.638514   -0.459322    0.259293   -0.000005   69.413888   \n",
       "3     -0.034670  2892.223032   -0.459490    0.515741    0.000013    3.949912   \n",
       "4     -0.033231  2524.451998   -0.472095    0.821683    0.000003   15.007591   \n",
       "..          ...          ...         ...         ...         ...         ...   \n",
       "195   -0.049275  4988.911095   -0.390495   -0.611365    0.000013   90.446884   \n",
       "196   -0.022201  1386.488040   -0.591700    3.480437   -0.000003   28.440451   \n",
       "197   -0.038789  3353.990490   -0.406398   -0.432799   -0.000001   39.679107   \n",
       "198   -0.022171  1690.748781   -0.644783    3.764329   -0.000003    5.870222   \n",
       "199   -0.037090  3536.945813   -0.488229    0.854156    0.000014    9.187654   \n",
       "\n",
       "     farid_b_35  farid_b_36  \n",
       "0     -0.789374  234.072874  \n",
       "1      0.051449   17.678393  \n",
       "2     -0.101721   14.866676  \n",
       "3     -0.348138  150.724222  \n",
       "4     -0.135635   33.490953  \n",
       "..          ...         ...  \n",
       "195    0.036639    5.246472  \n",
       "196   -0.031909    6.388110  \n",
       "197   -0.061182    8.115477  \n",
       "198   -0.255036  100.579733  \n",
       "199   -0.168201   48.223144  \n",
       "\n",
       "[200 rows x 108 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pandas.read_csv('img-features.csv') #import our training data from the csv file\n",
    "    \n",
    "x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
