{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4735c34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from feature_extraction.ipynb\n",
      "importing Jupyter notebook from Image.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, metrics, preprocessing, linear_model, neighbors, ensemble, neural_network\n",
    "# from \"feature_extraction.ipynb\" import get_feature_vector\n",
    "import import_ipynb\n",
    "from feature_extraction import get_feature_vector\n",
    "import glob\n",
    "import fleep\n",
    "from Image import Image\n",
    "import ast\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5fd6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_svm_classifier(csv_file, joblib_file):\n",
    "    '''\n",
    "        Train SVM classifier with training set (feature vectors) from csv file,\n",
    "        and load the trained model in .joblib file\n",
    "    '''\n",
    "    training_data = pandas.read_csv(csv_file) #import our training data from the csv file\n",
    "    \n",
    "    x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler.fit_transform(x) #scale the features in the interval [0:1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #80% for training, 20% for testing\n",
    "    \n",
    "    classifier = svm.SVC(kernel = 'poly')\n",
    "    classifier.fit(x_train, y_train) # fit svm classifier to the train data\n",
    "    y_prediction = classifier.predict(x_test)\n",
    "        \n",
    "#     accuracy = metrics.accuracy_score(y_test, y_prediction) # accuracy of our model: number of correct predictions / number of total predictions\n",
    "    print(\"Accuracy on train set: \", classifier.score(x_train, y_train))\n",
    "    print(\"Accuracy on test set: \", classifier.score(x_test, y_test))\n",
    "#     print(\"Accuracy on test set: \", accuracy)\n",
    "#     joblib.dump(classifier, joblib_file) # save classifier in the joblib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40d812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knn_classifier(csv_file, joblib_file):\n",
    "    '''\n",
    "        Train SVM classifier with training set (feature vectors) from csv file,\n",
    "        and load the trained model in .joblib file\n",
    "    '''\n",
    "    training_data = pandas.read_csv(csv_file) #import our training data from the csv file\n",
    "    \n",
    "    x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler.fit_transform(x) #scale the features in the interval [0:1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #80% for training, 20% for testing\n",
    "    \n",
    "    classifier = neighbors.KNeighborsClassifier(500)\n",
    "    classifier.fit(x_train, y_train) # fit svm classifier to the train data\n",
    "    y_prediction = classifier.predict(x_test)\n",
    "    \n",
    "    \n",
    "#     accuracy = metrics.accuracy_score(y_test, y_prediction) # accuracy of our model: number of correct predictions / number of total predictions\n",
    "    print(\"K-nearest neighbors classifier: \")\n",
    "    print(\"Accuracy on train set: \", classifier.score(x_train, y_train))\n",
    "    print(\"Accuracy on test set: \", classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c7101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_forest_classifier(csv_file, joblib_file):\n",
    "    '''\n",
    "        Train SVM classifier with training set (feature vectors) from csv file,\n",
    "        and load the trained model in .joblib file\n",
    "    '''\n",
    "    training_data = pandas.read_csv(csv_file) #import our training data from the csv file\n",
    "    \n",
    "    x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler.fit_transform(x) #scale the features in the interval [0:1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #80% for training, 20% for testing\n",
    "    \n",
    "    classifier = ensemble.RandomForestClassifier(max_depth=8, n_estimators=20, max_features=3)\n",
    "    classifier.fit(x_train, y_train) # fit svm classifier to the train data\n",
    "    y_prediction = classifier.predict(x_test)\n",
    "    \n",
    "    \n",
    "#     accuracy = metrics.accuracy_score(y_test, y_prediction) # accuracy of our model: number of correct predictions / number of total predictions\n",
    "    print(\"Random Forests classifier: \")\n",
    "    print(\"Accuracy on train set: \", classifier.score(x_train, y_train))\n",
    "    print(\"Accuracy on test set: \", classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2848e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_classifier(csv_file, joblib_file):\n",
    "    '''\n",
    "        Train SVM classifier with training set (feature vectors) from csv file,\n",
    "        and load the trained model in .joblib file\n",
    "    '''\n",
    "    training_data = pandas.read_csv(csv_file) #import our training data from the csv file\n",
    "    \n",
    "    x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler.fit_transform(x) #scale the features in the interval [0:1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #80% for training, 20% for testing\n",
    "    \n",
    "    classifier = neural_network.MLPClassifier(alpha=1, max_iter=1000)\n",
    "    classifier.fit(x_train, y_train) # fit svm classifier to the train data\n",
    "    y_prediction = classifier.predict(x_test)\n",
    "    \n",
    "    \n",
    "#     accuracy = metrics.accuracy_score(y_test, y_prediction) # accuracy of our model: number of correct predictions / number of total predictions\n",
    "    print(\"MLP NN classifier: \")\n",
    "    print(\"Accuracy on train set: \", classifier.score(x_train, y_train))\n",
    "    print(\"Accuracy on test set: \", classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3caa11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_update_features(image):\n",
    "    '''\n",
    "        Get feature vector of image object,\n",
    "        return image object with image.features updated\n",
    "    '''\n",
    "    feature_vector = get_feature_vector(image.name)\n",
    "    farid_r, farid_g, farid_b = feature_vector[0], feature_vector[1], feature_vector[2]\n",
    "    \n",
    "    farid_dict = {} # will have every feature in every channel\n",
    "    \n",
    "    counter = 1\n",
    "    for feature_value in farid_r:\n",
    "        feature_name = 'farid_r_{}'.format(counter)\n",
    "#         farid_dict[feature_name] = float(feature_value)\n",
    "        farid_dict[feature_name] = float(feature_value)\n",
    "        counter = counter + 1\n",
    "    counter = 1\n",
    "    for feature_value in farid_g:\n",
    "        feature_name = 'farid_g_{}'.format(counter)\n",
    "        farid_dict[feature_name] = float(feature_value)\n",
    "        counter = counter + 1\n",
    "    counter = 1\n",
    "    for feature_value in farid_b:\n",
    "        feature_name = 'farid_b_{}'.format(counter)\n",
    "        farid_dict[feature_name] = float(feature_value)\n",
    "        counter = counter + 1\n",
    "    \n",
    "    image.features.update(farid_dict)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba784be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_images_list(img_list):\n",
    "    '''\n",
    "        Updates image.feature for a list of images\n",
    "    '''\n",
    "    for image in img_list:\n",
    "        image = extract_update_features(image)\n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cfa074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_lists(dir_location):\n",
    "    img_names = glob.glob(\"{}/*\".format(dir_location)) # gets list of all images in image directory (/stego or /clean)\n",
    "    img_list = [] # list of image objects\n",
    "    for img_name in img_names:\n",
    "#         if find_file(file_name):  # try to find file, if file can be found:\n",
    "        img_extension = get_img_extension(img_name)\n",
    "        img_size = os.path.getsize(img_name)\n",
    "        new_image = Image(img_name, img_extension, img_size)  # create Image object\n",
    "#         img_extension = get_img_extension(img_name)  # get file type and file extension\n",
    "#         img_size = os.path.getsize(img_name)  # get file size\n",
    "#         new_image.update_img(img_extension, img_size)  # update new_file with new info\n",
    "        img_list.append(new_image)  # add image object to image list\n",
    "    return img_list\n",
    "dire = os.getcwd()+'\\\\images'\n",
    "# get_img_lists(dire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e333d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_extension(img_name):\n",
    "#     with open(img_name, 'rb') as img:\n",
    "#         img_info = fleep.get(img.read(128))\n",
    "#         img_extension = img_info.extension[0]\n",
    "#         img.close()\n",
    "    name, img_extension = os.path.splitext(img_name)\n",
    "    return img_extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599c207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dir_location):\n",
    "    '''\n",
    "        return two lists of image objects in '/stego' and '/clean' directories with updated features\n",
    "    '''\n",
    "    # get image object list for stego and clean\n",
    "    stego_images = get_img_lists(\"{}/stego-reduced\".format(dir_location))\n",
    "    clean_images = get_img_lists(\"{}/clean-reduced\".format(dir_location))\n",
    "    \n",
    "    #extract and update features for both lists\n",
    "    stego_with_features = extract_from_images_list(stego_images)\n",
    "    clean_with_features = extract_from_images_list(clean_images)\n",
    "    \n",
    "    return stego_with_features, clean_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a821fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_img_to_csv(stego_with_features, clean_with_features):\n",
    "    output_file = 'img-features.csv' # our csv output file\n",
    "\n",
    "    list_of_dicts = []\n",
    "    feature_types = []\n",
    "    \n",
    "    for img in stego_with_features:\n",
    "        temp_dict = {}\n",
    "        temp_dict['img_name'] = img.name\n",
    "        for feature_type, feature_list in img.features.items():\n",
    "            temp_dict[feature_type] = feature_list\n",
    "            if feature_type not in feature_types:\n",
    "                feature_types.append(feature_type)\n",
    "        temp_dict['class'] = 1\n",
    "        list_of_dicts.append(temp_dict)\n",
    "    \n",
    "    for img in clean_with_features:\n",
    "        temp_dict = {}\n",
    "        temp_dict['img_name'] = img.name\n",
    "        for feature_type, feature_list in img.features.items():\n",
    "            temp_dict[feature_type] = feature_list\n",
    "            if feature_type not in feature_types:\n",
    "                feature_types.append(feature_type)\n",
    "        temp_dict['class'] = 0\n",
    "        list_of_dicts.append(temp_dict)\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        # set fieldnames\n",
    "        fieldnames = ['img_name']\n",
    "        for feature_type in feature_types:\n",
    "            fieldnames.append(feature_type)\n",
    "        fieldnames.append('class')\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for dict_item in list_of_dicts:\n",
    "            writer.writerow(dict_item)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1df2c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dir_location):\n",
    "    '''\n",
    "        Extrat features, create, train, and test our model\n",
    "    '''\n",
    "    directory = \"{}/images\".format(dir_location)\n",
    "    \n",
    "    stego_with_features, clean_with_features = extract_features(directory)\n",
    "    \n",
    "    write_img_to_csv(stego_with_features, clean_with_features)\n",
    "    \n",
    "    create_svm_classifier('img-features.csv', 'img-svm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3950d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set:  0.5005202913631633\n",
      "Accuracy on test set:  0.49480249480249483\n"
     ]
    }
   ],
   "source": [
    "# train_model(os.getcwd())\n",
    "create_svm_classifier('img-features.csv', 'img-svm.joblib')\n",
    "# create_knn_classifier('img-features.csv', 'img-svm.joblib')\n",
    "# create_random_forest_classifier('img-features.csv', 'img-svm.joblib')\n",
    "# create_mlp_classifier('img-features.csv', 'img-svm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89750aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  0.5031185031185031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\base.py:253: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.3 when using version 0.20.4. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "loaded_model = joblib.load(\"img-lr-jessica.joblib\")\n",
    "\n",
    "training_data = pandas.read_csv('img-features.csv') #import our training data from the csv file\n",
    "x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "x = scaler.fit_transform(x) #scale the features in the interval [0:1]\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) #80% for training, 20% for testing\n",
    "\n",
    "print(\"Accuracy on test set: \", loaded_model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b0756ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages)\n",
      "ERROR: Invalid requirement: 'scikit-learn=0.19.2'\n",
      "Hint: = is not a valid operator. Did you mean == ?\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn=0.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e254cc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>farid_r_1</th>\n",
       "      <th>farid_r_2</th>\n",
       "      <th>farid_r_3</th>\n",
       "      <th>farid_r_4</th>\n",
       "      <th>farid_r_5</th>\n",
       "      <th>farid_r_6</th>\n",
       "      <th>farid_r_7</th>\n",
       "      <th>farid_r_8</th>\n",
       "      <th>farid_r_9</th>\n",
       "      <th>farid_r_10</th>\n",
       "      <th>...</th>\n",
       "      <th>farid_b_27</th>\n",
       "      <th>farid_b_28</th>\n",
       "      <th>farid_b_29</th>\n",
       "      <th>farid_b_30</th>\n",
       "      <th>farid_b_31</th>\n",
       "      <th>farid_b_32</th>\n",
       "      <th>farid_b_33</th>\n",
       "      <th>farid_b_34</th>\n",
       "      <th>farid_b_35</th>\n",
       "      <th>farid_b_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.117610</td>\n",
       "      <td>931.664708</td>\n",
       "      <td>0.196375</td>\n",
       "      <td>2.580318</td>\n",
       "      <td>-13.333019</td>\n",
       "      <td>10931.936584</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>1.409096</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>113.285946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300753</td>\n",
       "      <td>14.744447</td>\n",
       "      <td>-0.039054</td>\n",
       "      <td>3443.002789</td>\n",
       "      <td>-0.435233</td>\n",
       "      <td>0.059799</td>\n",
       "      <td>1.170623e-05</td>\n",
       "      <td>46.070723</td>\n",
       "      <td>-0.063086</td>\n",
       "      <td>7.877356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.241478</td>\n",
       "      <td>1914.863894</td>\n",
       "      <td>-0.122600</td>\n",
       "      <td>2.229304</td>\n",
       "      <td>-19.946512</td>\n",
       "      <td>19668.566825</td>\n",
       "      <td>0.078170</td>\n",
       "      <td>-0.738696</td>\n",
       "      <td>0.012616</td>\n",
       "      <td>232.841788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175333</td>\n",
       "      <td>8.317309</td>\n",
       "      <td>-0.057065</td>\n",
       "      <td>6188.807620</td>\n",
       "      <td>-0.356140</td>\n",
       "      <td>-1.064673</td>\n",
       "      <td>2.118808e-05</td>\n",
       "      <td>52.761690</td>\n",
       "      <td>0.038872</td>\n",
       "      <td>3.939216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035054</td>\n",
       "      <td>201.088956</td>\n",
       "      <td>0.657866</td>\n",
       "      <td>23.197017</td>\n",
       "      <td>-5.244294</td>\n",
       "      <td>2675.208450</td>\n",
       "      <td>-0.269379</td>\n",
       "      <td>3.740757</td>\n",
       "      <td>-0.001831</td>\n",
       "      <td>24.451241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659413</td>\n",
       "      <td>48.990810</td>\n",
       "      <td>-0.050558</td>\n",
       "      <td>5143.925948</td>\n",
       "      <td>-0.373919</td>\n",
       "      <td>-0.849749</td>\n",
       "      <td>6.487192e-06</td>\n",
       "      <td>4.248978</td>\n",
       "      <td>-0.138656</td>\n",
       "      <td>28.868942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.161205</td>\n",
       "      <td>491.256853</td>\n",
       "      <td>0.020983</td>\n",
       "      <td>15.069482</td>\n",
       "      <td>-12.773164</td>\n",
       "      <td>12029.228004</td>\n",
       "      <td>-0.145996</td>\n",
       "      <td>1.751877</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>59.736691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128606</td>\n",
       "      <td>70.671139</td>\n",
       "      <td>-0.044134</td>\n",
       "      <td>4568.487387</td>\n",
       "      <td>-0.448853</td>\n",
       "      <td>0.218712</td>\n",
       "      <td>1.123088e-05</td>\n",
       "      <td>6.535435</td>\n",
       "      <td>0.029716</td>\n",
       "      <td>42.165031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.160191</td>\n",
       "      <td>281.236138</td>\n",
       "      <td>0.138394</td>\n",
       "      <td>29.563047</td>\n",
       "      <td>-7.429747</td>\n",
       "      <td>3447.779958</td>\n",
       "      <td>-0.162200</td>\n",
       "      <td>3.894538</td>\n",
       "      <td>0.008369</td>\n",
       "      <td>34.199515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.364053</td>\n",
       "      <td>331.371977</td>\n",
       "      <td>-0.018899</td>\n",
       "      <td>934.762252</td>\n",
       "      <td>-0.564545</td>\n",
       "      <td>2.759619</td>\n",
       "      <td>3.203432e-07</td>\n",
       "      <td>1.181646</td>\n",
       "      <td>0.077636</td>\n",
       "      <td>201.991284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>-0.122719</td>\n",
       "      <td>571.887975</td>\n",
       "      <td>-0.290372</td>\n",
       "      <td>19.482851</td>\n",
       "      <td>-7.598255</td>\n",
       "      <td>6452.785644</td>\n",
       "      <td>-0.329063</td>\n",
       "      <td>4.350538</td>\n",
       "      <td>0.006412</td>\n",
       "      <td>69.539607</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543838</td>\n",
       "      <td>120.992250</td>\n",
       "      <td>-0.020872</td>\n",
       "      <td>1902.841016</td>\n",
       "      <td>-0.641468</td>\n",
       "      <td>3.478047</td>\n",
       "      <td>1.868390e-05</td>\n",
       "      <td>4.949605</td>\n",
       "      <td>0.120218</td>\n",
       "      <td>73.017074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>-0.015311</td>\n",
       "      <td>190.819437</td>\n",
       "      <td>0.272703</td>\n",
       "      <td>41.541720</td>\n",
       "      <td>-4.802315</td>\n",
       "      <td>1969.063531</td>\n",
       "      <td>-0.234632</td>\n",
       "      <td>3.117024</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>23.202416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115036</td>\n",
       "      <td>74.247965</td>\n",
       "      <td>-0.021430</td>\n",
       "      <td>2302.213828</td>\n",
       "      <td>-0.685358</td>\n",
       "      <td>4.134864</td>\n",
       "      <td>-1.641496e-06</td>\n",
       "      <td>7.076301</td>\n",
       "      <td>-0.024812</td>\n",
       "      <td>44.357991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>-0.199249</td>\n",
       "      <td>305.342124</td>\n",
       "      <td>0.099720</td>\n",
       "      <td>14.978776</td>\n",
       "      <td>-20.411398</td>\n",
       "      <td>23256.686289</td>\n",
       "      <td>0.014610</td>\n",
       "      <td>-0.218370</td>\n",
       "      <td>0.010410</td>\n",
       "      <td>37.132313</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226035</td>\n",
       "      <td>39.343578</td>\n",
       "      <td>-0.048312</td>\n",
       "      <td>4622.728977</td>\n",
       "      <td>-0.375460</td>\n",
       "      <td>-0.797586</td>\n",
       "      <td>9.397364e-06</td>\n",
       "      <td>5.321953</td>\n",
       "      <td>0.050280</td>\n",
       "      <td>22.960613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2401</th>\n",
       "      <td>-0.148812</td>\n",
       "      <td>232.047080</td>\n",
       "      <td>0.156240</td>\n",
       "      <td>31.993590</td>\n",
       "      <td>-10.402956</td>\n",
       "      <td>6524.482399</td>\n",
       "      <td>-0.064958</td>\n",
       "      <td>1.184954</td>\n",
       "      <td>0.007775</td>\n",
       "      <td>28.218031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229299</td>\n",
       "      <td>58.944709</td>\n",
       "      <td>-0.046596</td>\n",
       "      <td>4454.519837</td>\n",
       "      <td>-0.382499</td>\n",
       "      <td>-0.735140</td>\n",
       "      <td>2.113706e-05</td>\n",
       "      <td>8.136546</td>\n",
       "      <td>-0.044840</td>\n",
       "      <td>34.969698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2402</th>\n",
       "      <td>-0.297314</td>\n",
       "      <td>154.681280</td>\n",
       "      <td>-0.393273</td>\n",
       "      <td>40.877016</td>\n",
       "      <td>-22.334693</td>\n",
       "      <td>22602.800184</td>\n",
       "      <td>0.127901</td>\n",
       "      <td>-1.176960</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>18.818733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495977</td>\n",
       "      <td>229.789194</td>\n",
       "      <td>-0.040410</td>\n",
       "      <td>3188.581549</td>\n",
       "      <td>-0.394804</td>\n",
       "      <td>-0.324330</td>\n",
       "      <td>1.839388e-05</td>\n",
       "      <td>5.109950</td>\n",
       "      <td>-0.101203</td>\n",
       "      <td>139.692923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2403 rows Ã— 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      farid_r_1    farid_r_2  farid_r_3  farid_r_4  farid_r_5     farid_r_6  \\\n",
       "0     -0.117610   931.664708   0.196375   2.580318 -13.333019  10931.936584   \n",
       "1     -0.241478  1914.863894  -0.122600   2.229304 -19.946512  19668.566825   \n",
       "2      0.035054   201.088956   0.657866  23.197017  -5.244294   2675.208450   \n",
       "3     -0.161205   491.256853   0.020983  15.069482 -12.773164  12029.228004   \n",
       "4     -0.160191   281.236138   0.138394  29.563047  -7.429747   3447.779958   \n",
       "...         ...          ...        ...        ...        ...           ...   \n",
       "2398  -0.122719   571.887975  -0.290372  19.482851  -7.598255   6452.785644   \n",
       "2399  -0.015311   190.819437   0.272703  41.541720  -4.802315   1969.063531   \n",
       "2400  -0.199249   305.342124   0.099720  14.978776 -20.411398  23256.686289   \n",
       "2401  -0.148812   232.047080   0.156240  31.993590 -10.402956   6524.482399   \n",
       "2402  -0.297314   154.681280  -0.393273  40.877016 -22.334693  22602.800184   \n",
       "\n",
       "      farid_r_7  farid_r_8  farid_r_9  farid_r_10  ...  farid_b_27  \\\n",
       "0     -0.081772   1.409096   0.006145  113.285946  ...    0.300753   \n",
       "1      0.078170  -0.738696   0.012616  232.841788  ...   -0.175333   \n",
       "2     -0.269379   3.740757  -0.001831   24.451241  ...    0.659413   \n",
       "3     -0.145996   1.751877   0.008422   59.736691  ...   -0.128606   \n",
       "4     -0.162200   3.894538   0.008369   34.199515  ...   -0.364053   \n",
       "...         ...        ...        ...         ...  ...         ...   \n",
       "2398  -0.329063   4.350538   0.006412   69.539607  ...   -0.543838   \n",
       "2399  -0.234632   3.117024   0.000800   23.202416  ...    0.115036   \n",
       "2400   0.014610  -0.218370   0.010410   37.132313  ...   -0.226035   \n",
       "2401  -0.064958   1.184954   0.007775   28.218031  ...    0.229299   \n",
       "2402   0.127901  -1.176960   0.015533   18.818733  ...    0.495977   \n",
       "\n",
       "      farid_b_28  farid_b_29   farid_b_30  farid_b_31  farid_b_32  \\\n",
       "0      14.744447   -0.039054  3443.002789   -0.435233    0.059799   \n",
       "1       8.317309   -0.057065  6188.807620   -0.356140   -1.064673   \n",
       "2      48.990810   -0.050558  5143.925948   -0.373919   -0.849749   \n",
       "3      70.671139   -0.044134  4568.487387   -0.448853    0.218712   \n",
       "4     331.371977   -0.018899   934.762252   -0.564545    2.759619   \n",
       "...          ...         ...          ...         ...         ...   \n",
       "2398  120.992250   -0.020872  1902.841016   -0.641468    3.478047   \n",
       "2399   74.247965   -0.021430  2302.213828   -0.685358    4.134864   \n",
       "2400   39.343578   -0.048312  4622.728977   -0.375460   -0.797586   \n",
       "2401   58.944709   -0.046596  4454.519837   -0.382499   -0.735140   \n",
       "2402  229.789194   -0.040410  3188.581549   -0.394804   -0.324330   \n",
       "\n",
       "        farid_b_33  farid_b_34  farid_b_35  farid_b_36  \n",
       "0     1.170623e-05   46.070723   -0.063086    7.877356  \n",
       "1     2.118808e-05   52.761690    0.038872    3.939216  \n",
       "2     6.487192e-06    4.248978   -0.138656   28.868942  \n",
       "3     1.123088e-05    6.535435    0.029716   42.165031  \n",
       "4     3.203432e-07    1.181646    0.077636  201.991284  \n",
       "...            ...         ...         ...         ...  \n",
       "2398  1.868390e-05    4.949605    0.120218   73.017074  \n",
       "2399 -1.641496e-06    7.076301   -0.024812   44.357991  \n",
       "2400  9.397364e-06    5.321953    0.050280   22.960613  \n",
       "2401  2.113706e-05    8.136546   -0.044840   34.969698  \n",
       "2402  1.839388e-05    5.109950   -0.101203  139.692923  \n",
       "\n",
       "[2403 rows x 108 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pandas.read_csv('img-features.csv') #import our training data from the csv file\n",
    "    \n",
    "x, y = training_data.drop(['img_name', 'class'], axis=1), training_data['class']\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
