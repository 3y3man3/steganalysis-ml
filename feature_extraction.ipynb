{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ac0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.stats import skew, kurtosis\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "# import pywt\n",
    "# from sklearn import preprocessing\n",
    "# # import pysteg\n",
    "\n",
    "\n",
    "# def get_feature_vector(img_path):\n",
    "#     '''\n",
    "#         return the extracted feature vector by applying\n",
    "#         a High Order Statistical Steganalysis (Farid Technique) on RGB seperated channels\n",
    "#     '''\n",
    "    img = cv2.imread(img_path) # read image using opencv\n",
    "    \n",
    "#     # copy file 3 times for each colour channel (red, green, blue)\n",
    "    # img_red = img.copy()\n",
    "    # img_green = img.copy()\n",
    "    # img_blue = img.copy()\n",
    "\n",
    "#     # isolate colour channels\n",
    "    # img_red[:, :, 1] = 0\n",
    "    # img_red[:, :, 2] = 0\n",
    "    # img_green[:, :, 0] = 0\n",
    "#     img_green[:, :, 2] = 0\n",
    "#     img_blue[:, :, 0] = 0\n",
    "#     img_blue[:, :, 1] = 0\n",
    "        \n",
    "#     # get features vectors for each colour channel\n",
    "    # colour_channels = [img_red, img_green, img_blue]\n",
    "#     feature_vector = []\n",
    "    # for channel in colour_channels:\n",
    "    #     feature_vector.append(farid36(channel,\"haar\"))\n",
    "#     return feature_vector\n",
    "\n",
    "\n",
    "# def farid36(I, name=\"qmf9\",*a,**kw):\n",
    "#     \"\"\"\n",
    "#     Calculate the 36 Farid features from the image I\n",
    "#     Optionally a wavelet name can be given as well.\n",
    "#     \"\"\"\n",
    "#     wavelet = get_wavelet(name) \n",
    "#     H = pywt.wavedec2( I, wavelet, level=4 )\n",
    "# #     print(H)\n",
    "# #     return H\n",
    "#     return farid36aux(H)\n",
    "\n",
    "\n",
    "# def farid36aux(H):\n",
    "#     R = []\n",
    "#     for h in H[2:]:\n",
    "#         for A in h:\n",
    "# #             print(\"AAAAAAAAAAAAAA\")\n",
    "# #             print(skew(A,axis=None))\n",
    "#             R.extend([np.mean(A), np.var(A), skew(A, axis=None), kurtosis(A, axis=None)])\n",
    "# #             R.extend([np.mean(A), np.var(A)])\n",
    "#     return R\n",
    "\n",
    "# def get_wavelet(name = \"qmf9\"):\n",
    "#     ''' Making a QMF-9 wavelet object in Python '''\n",
    "#     lf = [ 0.02807382, -0.060944743, -0.073386624, 0.41472545, 0.7973934,\n",
    "#             0.41472545, -0.073386624, -0.060944743, 0.02807382, 0 ]\n",
    "#     size = len(lf)\n",
    "#     size2 = np.ceil(float(size)/2) - 1\n",
    "#     hf = []\n",
    "# #     for i in range(size-1,-1,-1):\n",
    "# #         hf.append(lf[i]*(-1)**(i-size2))\n",
    "#     hf = [ lf[i]*(-1)**(i-size2) for i in range(size-1,-1,-1) ]\n",
    "#     f = ( lf, hf, list(reversed(lf)), list(reversed(hf)) )\n",
    "#     qmf9 = pywt.Wavelet(name, f)\n",
    "#     return qmf9\n",
    "\n",
    "# get_wavelet()\n",
    "# get_feature_vector(\"Lionel_Messi_512_769.jpg\")\n",
    "# scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "# # x = scaler.fit_transform(x)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9874a929-8304-4a14-89bb-45618afd7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pywt\n",
    "from skimage import io\n",
    "\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "# from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99554250-9b03-4ec0-bbbf-ad11ebfa0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_metrics(x):\n",
    "    \"\"\"\n",
    "    Calculates statistical metrics on input array (mean, std, skew, kurtosis).\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        'mean': np.mean,\n",
    "        'stdev': np.std,\n",
    "        'skew': stats.skew,\n",
    "        'kurtosis': stats.kurtosis\n",
    "    }\n",
    "    return {k: fn(x.flatten()) for k, fn in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05398297-824b-481e-8cbc-82173c7db373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_dict_keys(d, prefix):\n",
    "    \"\"\"\n",
    "    Adds prefix to dict keys.\n",
    "    \"\"\"\n",
    "    return {'{}_{}'.format(prefix, k): v for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59459239-c759-4d5a-af5e-2e60d85766f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation_features(I, lags=[(1, 0), (0, 1), (1, 1)]):\n",
    "    \"\"\"\n",
    "    Calculate the autocorrelation statistical features from a 2D image array\n",
    "    (greyscale image or an individual colour channel) for the specified pixel\n",
    "    vertical and horizontal coordinate shift lags:\n",
    "        e.g. [(1, 0), (0, 1), (1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "    \"\"\"\n",
    "    m, n = I.shape\n",
    "\n",
    "    features = {}\n",
    "    for x, y in lags:\n",
    "        ac = I[x:, y:] * I[:m-x, :n-y]\n",
    "        aca = np.sum(ac / (I[x:, y:].std() * I[:m-x, :n-y].std()))\n",
    "\n",
    "        features['aca_{}{}'.format(x, y)] = aca\n",
    "\n",
    "        f_stat = statistical_metrics(ac)\n",
    "        f_stat = prefix_dict_keys(f_stat, 'ac_{}{}'.format(x, y))\n",
    "        features.update(f_stat)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def rgb_autocorrelation_features(I, lags=((1, 0), (0, 1), (1, 1))):\n",
    "    \"\"\"\n",
    "    Calculate the autocorrelation statistical features of an RGB image\n",
    "    array (m, n, 3) for the specified lags.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    m, n, _ = I.shape\n",
    "\n",
    "    for c, colour in enumerate('rgb'):\n",
    "        f_ac = autocorrelation_features(I[:, :, c], lags)\n",
    "        f_ac = prefix_dict_keys(f_ac, colour)\n",
    "        features.update(f_ac)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def concatenate_feature_sets(filepath_cover, filepath_stego, filepath_output):\n",
    "    \"\"\"\n",
    "    Concatenates two feature csv files.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath_cover : string\n",
    "        Filepath to cover image feature set.\n",
    "    filepath_stego : string\n",
    "        Filepath to steganographic image feature set.\n",
    "    filepath_output : string\n",
    "        Output filepath.\n",
    "    Returns\n",
    "    -------\n",
    "    Concatenated dataset.\n",
    "    \"\"\"\n",
    "    train_cover = pd.read_csv(filepath_cover)\n",
    "    train_stego = pd.read_csv(filepath_stego)\n",
    "    train = pd.concat([train_cover, train_stego])\n",
    "    train.to_csv(filepath_output, index=False)\n",
    "    return train\n",
    "\n",
    "\n",
    "def concat_multiple_feature_sets(filepaths, filepath_output):\n",
    "    train = pd.DataFrame()\n",
    "    for filepath in filepaths:\n",
    "        df = pd.read_csv(filepath)\n",
    "        df['filename'] = filepath.split('/')[-1]\n",
    "        train = pd.concat([train, df])\n",
    "    train.to_csv(filepath_output, index=False)\n",
    "    return train\n",
    "\n",
    "\n",
    "def apply_tolerance(x, tol):\n",
    "    \"\"\"\n",
    "    Applies absolute value filter for given tolerance.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Input data.\n",
    "    tol : int, float\n",
    "        Tolerance.\n",
    "    Returns\n",
    "    -------\n",
    "    Filtered array where |x| >= tol.\n",
    "    If no values are above the tolerance np.array([0]) is returned.\n",
    "    \"\"\"\n",
    "    x_tol = abs(x) >= tol\n",
    "    if x_tol.any():\n",
    "        return x[x_tol]\n",
    "    else:\n",
    "        return np.zeros(1)\n",
    "\n",
    "\n",
    "def wavdec_features(coeffs, tol=1):\n",
    "    \"\"\"\n",
    "    Calculated the statistical features on the components of a mulitlevel 2D\n",
    "    discrete wavelet decomposition.\n",
    "    Parameters\n",
    "    ----------\n",
    "    coeffs : list\n",
    "        n level coefficients from pywt.wavedec2\n",
    "        [cAn, (cHn, cVn, cDn), ... (cH1, cV1, cD1)]\n",
    "    tol : int, float, default : 1\n",
    "        Tolerance to apply to individual coefficient arrays.\n",
    "    Returns\n",
    "    -------\n",
    "    features : dict\n",
    "        Feature vector of statistical components in dictionary form.\n",
    "    \"\"\"\n",
    "    n_layers = len(coeffs) - 1\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    cA = coeffs[0]\n",
    "    prefix = 'dwt_{}_cA'.format(n_layers)\n",
    "    cA = apply_tolerance(cA, tol)  # reduce sensitivity to noise\n",
    "    f_stat = statistical_metrics(cA)\n",
    "    f_stat = prefix_dict_keys(f_stat, prefix)\n",
    "    features.update(f_stat)\n",
    "\n",
    "    for i, (cH, cV, cD) in enumerate(coeffs[1:]):\n",
    "        layer = n_layers - i\n",
    "        for c, cX in zip(('cH', 'cV', 'cD'), (cH, cV, cD)):\n",
    "            prefix = 'dwt_{}_{}'.format(layer, c)\n",
    "            cX = apply_tolerance(cX, tol)\n",
    "            f_stat = statistical_metrics(cX)\n",
    "            f_stat = prefix_dict_keys(f_stat, prefix)\n",
    "            features.update(f_stat)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def rgb_wavelet_features(I, tol=1):\n",
    "    \"\"\"\n",
    "    For each RGB channel, calculates the statistical features the components of\n",
    "    a mulitlevel 2D discrete wavelet decomposition.\n",
    "    Parameters\n",
    "    ----------\n",
    "    I : numpy.ndarray\n",
    "        RGB image array.\n",
    "    tol : int, float, default : 1\n",
    "        Tolerance to apply to individual coefficient arrays.\n",
    "    Returns\n",
    "    -------\n",
    "    features : dict\n",
    "        Feature vector of statistical components in dictionary form.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    m, n, _ = I.shape\n",
    "\n",
    "    for c, colour in enumerate('rgb'):\n",
    "        coeffs = pywt.wavedec2(I[:, :, c], wavelet='haar', level=3)\n",
    "        f_wavelet = wavdec_features(coeffs)\n",
    "        f_wavelet = prefix_dict_keys(f_wavelet, colour)\n",
    "        features.update(f_wavelet)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def create_feature_dataset(path_images, class_label, path_output,\n",
    "                           f_types=['autocorrelation', 'wavelet'],\n",
    "                           image_limit=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Create feature vectors from images in directory and save as csv output.\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_images : directory path string\n",
    "        Directory with images for processing.\n",
    "    class_label : string\n",
    "        Class label used in label column of output.\n",
    "    path_output : directory path string\n",
    "        Output directory for csv file.\n",
    "    f_types : array_like, default : ['autocorrelation', 'wavelet']\n",
    "        Specify the feature types to include as list of strings:\n",
    "        {'autocorrelation', 'wavelet'}\n",
    "        Default: ['autocorrelation', 'wavelet']\n",
    "    image_limit : int, default : None\n",
    "        Number of images in directory to process.\n",
    "    Returns\n",
    "    -------\n",
    "    csv output file as specified in path_output.\n",
    "    \"\"\"\n",
    "\n",
    "    print('creating image feature dataset...')\n",
    "\n",
    "    dataset = list()\n",
    "    for i, filename in enumerate(os.listdir(path_images)):\n",
    "        fname = '{}{}'.format(path_images, filename)\n",
    "        image = io.imread(fname)\n",
    "\n",
    "        features = {}\n",
    "        if 'autocorrelation' in f_types:\n",
    "            lags = ((1, 0), (0, 1), (1, 1), (1, 2), (2, 2), (2, 2))\n",
    "            features.update(rgb_autocorrelation_features(image, lags))\n",
    "\n",
    "        if 'wavelet' in f_types:\n",
    "            features.update(rgb_wavelet_features(image))\n",
    "\n",
    "        if i == 0:\n",
    "            feature_names = features.keys()\n",
    "\n",
    "        row = [filename, class_label]\n",
    "        for feature in feature_names:\n",
    "            row.append(features[feature])\n",
    "        dataset.append(row)\n",
    "\n",
    "        if i % 250 == 0:\n",
    "            print('{} images processed'.format(i))\n",
    "\n",
    "        if image_limit:\n",
    "            if i > image_limit:\n",
    "                break\n",
    "\n",
    "    df = pd.DataFrame(dataset, columns=['image', 'label'] + list(feature_names))\n",
    "    df.to_csv(path_output, index=False)\n",
    "\n",
    "    print('image feature dataset created.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d063b8a-2465-4393-84b1-b984829bf692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating image feature dataset...\n",
      "0 images processed\n",
      "250 images processed\n",
      "500 images processed\n",
      "750 images processed\n",
      "1000 images processed\n",
      "image feature dataset created.\n"
     ]
    }
   ],
   "source": [
    "input_dir = os.getcwd()+'\\\\images\\\\clean\\\\'\n",
    "# input_dir\n",
    "create_feature_dataset(input_dir, 0, 'clean_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bec33d7-e240-4d9c-ac69-d43b31acf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv('clean_features.csv'), pd.read_csv('stego_features.csv')])\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"img-features.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cfdfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install scikit-image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
